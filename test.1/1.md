

```python
import math
import operator


def createDataSet():
    dataSet = [[0, 0, 0, 0, 'no'],
               [0, 0, 0, 1, 'no'],
               [0, 1, 0, 1, 'yes'],
               [0, 1, 1, 0, 'yes'],
               [0, 0, 0, 0, 'no'],
               [1, 0, 0, 0, 'no'],
               [1, 0, 0, 1, 'no'],
               [1, 1, 1, 1, 'yes'],
               [1, 0, 1, 2, 'yes'],
               [1, 0, 1, 2, 'yes'],
               [2, 0, 1, 2, 'yes'],
               [2, 0, 1, 1, 'yes'],
               [2, 1, 0, 1, 'yes'],
               [2, 1, 0, 2, 'yes'],
               [2, 0, 0, 0, 'no']]
    labels = ['F1-AGE', 'F2-WORK', 'F3-HOME', 'F4_LOAN']
    return dataSet, labels


def createTree(dataSet, labels, featLabels):
    # 1.创建树模型
    classList = [example[-1] for example in dataSet]  # 将dataSet的最后一个属性的分类标签储存到classList中
    if classList.count(classList[0]) == len(classList):  # 停止条件：计算分类标签第一个的个数，如果是和总数一样，证明只 有第一个，类似于全是yes或全是No
        return classList[0]  # 返回唯一的那个值 yes或no
    # 如果dataSet只剩下一列标签，证明遍历完当前数据集
    if len(dataSet[0]) == 1:
        return majorityCnt(classList)  # 函数-->计算当前节点中最多类别的（多数服从少数）
    bestfeat = chooseBestFeatureToSplit(dataSet)  # 找到当前最适合分裂数据的特征索引
    bestfeatLable = labels[bestfeat]  # 获取新节点的标签名称
    featLabels.append(bestfeatLable)  # 将最佳特征的标签名称添加到featLabels中
    myTree = {bestfeatLable: {}}  # 创建当前节点（字典类型）eg{‘F1-AGE’:{}}
    del labels[bestfeat]  # 从labels中删除该特征，表示它已经被使用过了
    sub_labels = labels[:]  # 深度拷贝 sub_labels----> 新的特征列表
    featValue = [example[bestfeat] for example in dataSet]  # 获取最佳特征的域（所有可能取值）
    uniqueVals = set(featValue)  # set方法获得唯一的值，还能计算出个数---->去重
    for value in uniqueVals:
        # 调用splitDataSet方法按每个取值分裂数据集
        myTree[bestfeatLable][value] = createTree(splitDataSet(dataSet,bestfeat,value),sub_labels,featLabels)  # 递归

    return myTree


# 找出多数类别
def majorityCnt(classList):
    classCount={}  # 计数
    for vote in classList:  # 统计类别出现次数并排序，返回出现次数最多的类被
        if vote not in classCount.keys():  # 相同的遍历方法
            classCount[vote]=0  # 未出现的初始值为0
        classCount[vote] += 1  # 出现过就加一
    sortedclassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)  # 排序-->classCount.items（）：返回字典试图；key：指定排序的依据；operator.itemgetter(1):是一个函数，它从每个元组（即字典中的键值对）中提取索引 1 的 元素。在字典的条目中，索引 1 对应的是值;reverse=True:从小到大的顺序
    return sortedclassCount[0][0]


# 挑选最佳分裂特征
def chooseBestFeatureToSplit(dataSet):
    numFeatures = len(dataSet[0]) - 1  # 特征数量
    baseEntropy = calcshannonEnt(dataSet)  # 原始数据集的信息熵
    bestInfoGain = 0  # 记录最高的信息增益
    bestFeature = -1  # 记录最佳特征索引
    for i in range(numFeatures):  # 获取每个特征的索引取值
        featList = [example[i] for example in dataSet]  # 获取当前特征索引一列的所有可能
        uniqueVals = set(featList)  # 去重
        newEntropy = 0  # 按特征分裂后新的熵值
        for val in uniqueVals:  # 获取域（所有取值）
            subDataSet = splitDataSet(dataSet, i, val)  # 经过切分之后的数据集,去除特征索引为i的那一列，并返回该特征的值为val的一行，也就是符合要求的数据，eg：splitDataSet(dataSet, 1, 'low')，去除第二列，并返回第二列中值为low的数据行
            prob = len(subDataSet)/float(len(dataSet))  # 符合条件的行数/原数据集的行数-->概率
            newEntropy += prob * calcshannonEnt(subDataSet) # 新熵为该特征值下的所有取值的概率*熵，subDataSet为符合我们 筛选的数据
        infoGain = baseEntropy - newEntropy  # 信息增益：H原始-H分裂后
        if (infoGain > bestInfoGain):
            bestInfoGain = infoGain
            bestFeature = i
    return bestFeature  # 返回信息增益最大的特征索引


# 划分数据集
def splitDataSet(dataSet, axis, val):  # dataSet：包含多个数据点的列表；axis：要划分的特征的索引；val：根据特定值划分数 据集
    retDataSet = []  # 创建空列表
    for featvec in dataSet:  # 遍历原数据集
        if featvec[axis] == val:  # 检查它在axis位置的值是否等于val，等于之后
            reducedfeatvec = featvec[:axis]  # 去除axis位置上的特征值，获取axis列之前的所有特征值
            reducedfeatvec.extend(featvec[axis+1:]) # 获取axis之后的所有特征值，将后面的特征获取到reducedfeatvec
            retDataSet.append(reducedfeatvec)  # 将reducedfeatvec写入retDataSet
    return retDataSet


# 计算信息熵
def calcshannonEnt(dataSet):
    numexamples = len(dataSet)  # 获取数据集的条数
    labelCounts = {}  # 统计每个分类标签的出现次数，字典类型key：value
    for featVec in dataSet:  # 遍历dataSet的每一条
        currentlabel = featVec[-1]  # 现在的类别
        if currentlabel not in labelCounts:  # 如果现在的类别在labelCounts字典中没有出现过，则初始化为0，然后+1，如果出 现过，直接+1
            labelCounts[currentlabel] = 0
        labelCounts[currentlabel] += 1

    shannonEnt = 0  # 初始化熵为0
    for key in labelCounts:  # 获取字典中的key
        prop = float(labelCounts[key])/numexamples # 获取概率  用labelCounts[key]（该类别出现的次数）/numexamples（总条 数）
        shannonEnt -= prop*math.log(prop, 2)  # 熵公式
    return shannonEnt


if __name__ == '__main__':
    dataSet,labels = createDataSet()
    featLabels = []
    myTree = createTree(dataSet, labels, featLabels)  # myTree是一个嵌套的字典结构
    print("Generated Decision Tree:")
    print(myTree)
```
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTExNDIyOTU3ODEsMzE3NjgyOTQ2XX0=
-->